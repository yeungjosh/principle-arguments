{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "\n",
    "import model as m\n",
    "from utils import pad_sents, data_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/albertxu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/content/drive/My Drive/577-project/reason-dataset.csv')\n",
    "df = pickle.load(open('reasons_complete_df.pkl', 'rb'))\n",
    "df['lower_line'] = df['line'].apply(lambda r: r.lower()).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoPAs = pd.read_csv('principle_argument_CoPA/PA_list.txt', header=None, sep='$', squeeze=True)\n",
    "copas_t = CoPAs.apply(lambda row: row.lower()).apply(word_tokenize)\n",
    "\n",
    "copa_w2v = pickle.load(open('principle_argument_CoPA/PA_embeds_avg_w2v.pkl', 'rb'))\n",
    "copa_tfidf = pickle.load(open('principle_argument_CoPA/PA_embeds_tfidf_df.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "copa_tfidf['CoPA_embedding'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>topic</th>\n",
       "      <th>stance</th>\n",
       "      <th>substance</th>\n",
       "      <th>source</th>\n",
       "      <th>line</th>\n",
       "      <th>claim_tf_idf_embedding</th>\n",
       "      <th>tf_idf_pa_similarity</th>\n",
       "      <th>claim_SBERT_embedding</th>\n",
       "      <th>lower_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Con</td>\n",
       "      <td>c-adopt</td>\n",
       "      <td>Q34</td>\n",
       "      <td>And if it is not possible for your to have a b...</td>\n",
       "      <td>[227.53242, 95.16571, 90.81468, 413.58224, -44...</td>\n",
       "      <td>[0.48416767, 0.4241976, 0.54075974, 0.69915974...</td>\n",
       "      <td>[0.021533493, -0.16520593, 0.35402358, 0.43329...</td>\n",
       "      <td>[and, if, it, is, not, possible, for, your, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Con</td>\n",
       "      <td>c-kill</td>\n",
       "      <td>Q34</td>\n",
       "      <td>I believe that abortion cannot be justified be...</td>\n",
       "      <td>[32.17483, 92.68718, 283.92633, 117.998245, -2...</td>\n",
       "      <td>[0.47884166, 0.43939564, 0.5063248, 0.70720005...</td>\n",
       "      <td>[0.47159326, 0.4925952, -0.23349553, -0.087148...</td>\n",
       "      <td>[i, believe, that, abortion, can, not, be, jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Con</td>\n",
       "      <td>c-baby_right</td>\n",
       "      <td>Q34</td>\n",
       "      <td>Moreover, United Nations declaration says chil...</td>\n",
       "      <td>[-27.021324, -21.564283, 46.234787, 301.42996,...</td>\n",
       "      <td>[0.43071118, 0.45206025, 0.48450536, 0.5497553...</td>\n",
       "      <td>[0.4615807, 0.68479395, 0.737707, 0.26567188, ...</td>\n",
       "      <td>[moreover, ,, united, nations, declaration, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Con</td>\n",
       "      <td>c-baby_right</td>\n",
       "      <td>M25</td>\n",
       "      <td>Even if it doesn't have a brain, my belief is ...</td>\n",
       "      <td>[357.5555, -13.070822, 262.6633, 333.12784, -2...</td>\n",
       "      <td>[0.44407952, 0.4239362, 0.49705744, 0.70402575...</td>\n",
       "      <td>[0.78423434, 0.56576806, 0.69618547, 0.0808582...</td>\n",
       "      <td>[even, if, it, does, n't, have, a, brain, ,, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>abortion</td>\n",
       "      <td>Con</td>\n",
       "      <td>c-sex</td>\n",
       "      <td>O43</td>\n",
       "      <td>Can't you use a condom while having sex. Yes, ...</td>\n",
       "      <td>[182.9457, 77.004105, 34.06093, 160.79187, -15...</td>\n",
       "      <td>[0.41161808, 0.39982748, 0.43759474, 0.6643834...</td>\n",
       "      <td>[-0.19426306, 0.5660061, -0.13704692, -0.07844...</td>\n",
       "      <td>[ca, n't, you, use, a, condom, while, having, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     topic stance     substance source  \\\n",
       "0   0  abortion    Con       c-adopt    Q34   \n",
       "1   1  abortion    Con        c-kill    Q34   \n",
       "2   2  abortion    Con  c-baby_right    Q34   \n",
       "3   3  abortion    Con  c-baby_right    M25   \n",
       "4   4  abortion    Con         c-sex    O43   \n",
       "\n",
       "                                                line  \\\n",
       "0  And if it is not possible for your to have a b...   \n",
       "1  I believe that abortion cannot be justified be...   \n",
       "2  Moreover, United Nations declaration says chil...   \n",
       "3  Even if it doesn't have a brain, my belief is ...   \n",
       "4  Can't you use a condom while having sex. Yes, ...   \n",
       "\n",
       "                              claim_tf_idf_embedding  \\\n",
       "0  [227.53242, 95.16571, 90.81468, 413.58224, -44...   \n",
       "1  [32.17483, 92.68718, 283.92633, 117.998245, -2...   \n",
       "2  [-27.021324, -21.564283, 46.234787, 301.42996,...   \n",
       "3  [357.5555, -13.070822, 262.6633, 333.12784, -2...   \n",
       "4  [182.9457, 77.004105, 34.06093, 160.79187, -15...   \n",
       "\n",
       "                                tf_idf_pa_similarity  \\\n",
       "0  [0.48416767, 0.4241976, 0.54075974, 0.69915974...   \n",
       "1  [0.47884166, 0.43939564, 0.5063248, 0.70720005...   \n",
       "2  [0.43071118, 0.45206025, 0.48450536, 0.5497553...   \n",
       "3  [0.44407952, 0.4239362, 0.49705744, 0.70402575...   \n",
       "4  [0.41161808, 0.39982748, 0.43759474, 0.6643834...   \n",
       "\n",
       "                               claim_SBERT_embedding  \\\n",
       "0  [0.021533493, -0.16520593, 0.35402358, 0.43329...   \n",
       "1  [0.47159326, 0.4925952, -0.23349553, -0.087148...   \n",
       "2  [0.4615807, 0.68479395, 0.737707, 0.26567188, ...   \n",
       "3  [0.78423434, 0.56576806, 0.69618547, 0.0808582...   \n",
       "4  [-0.19426306, 0.5660061, -0.13704692, -0.07844...   \n",
       "\n",
       "                                          lower_line  \n",
       "0  [and, if, it, is, not, possible, for, your, to...  \n",
       "1  [i, believe, that, abortion, can, not, be, jus...  \n",
       "2  [moreover, ,, united, nations, declaration, sa...  \n",
       "3  [even, if, it, does, n't, have, a, brain, ,, m...  \n",
       "4  [ca, n't, you, use, a, condom, while, having, ...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/Users/albertxu/577-Project/model.py'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = m.MatmulT(100, dropout=.3)\n",
    "#tr = m.LinearT(100, len(copas), dropout=.3)\n",
    "model = m.Model(5, 10, 2, tr, dropout=.3, embed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model(torch.tensor([[1,1,2,3,1],[2,2,2,3,1]], dtype=float), torch.tensor([[1,1,2,3,1],[2,2,2,3,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.2500, 0.5000, 0.7500, 0.2500],\n",
       "        [0.4264, 0.4264, 0.4264, 0.6396, 0.2132]], dtype=torch.float64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(z/torch.norm(z, dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1403, -0.1304, -0.0560, -0.1456, -0.0682,  0.0268,  0.1162,\n",
       "            0.0770, -0.1151,  0.1242, -0.1999, -0.1103, -0.3527,  0.1362,\n",
       "           -0.2672,  0.1425,  0.0383,  0.0856, -0.0584,  0.1248],\n",
       "          [-0.1756, -0.1810, -0.0847, -0.1967, -0.1038,  0.0636,  0.1584,\n",
       "            0.1186, -0.1596,  0.1921, -0.1815, -0.1360, -0.3064,  0.1273,\n",
       "           -0.2501,  0.1218,  0.0742,  0.0742, -0.0705,  0.1296],\n",
       "          [-0.0999, -0.1565, -0.0822,  0.0034, -0.1131, -0.0632,  0.2364,\n",
       "            0.1921, -0.2560, -0.0390, -0.1688, -0.3564, -0.1007,  0.1872,\n",
       "           -0.1550,  0.1140,  0.1996,  0.0561, -0.1113,  0.0832],\n",
       "          [-0.0849, -0.0482,  0.1167,  0.1129,  0.1532, -0.1485,  0.1242,\n",
       "            0.0247, -0.0558, -0.1259, -0.1755, -0.0476, -0.0557, -0.0165,\n",
       "            0.1652,  0.1688,  0.0959,  0.0956, -0.0610,  0.3632],\n",
       "          [-0.1661, -0.1376,  0.0195, -0.0819, -0.0258, -0.0465,  0.1400,\n",
       "            0.1331, -0.1401, -0.0166, -0.1386, -0.0523, -0.2035,  0.0392,\n",
       "           -0.1255,  0.0959,  0.0242,  0.0147, -0.0272,  0.0654]],\n",
       " \n",
       "         [[-0.0402, -0.1058, -0.0190,  0.0332, -0.0584, -0.0986,  0.1281,\n",
       "            0.1278, -0.2204, -0.1219, -0.1699, -0.4093, -0.1398,  0.3502,\n",
       "           -0.3513,  0.1443,  0.2175,  0.0429, -0.1301,  0.0518],\n",
       "          [-0.0436, -0.1344, -0.0331,  0.0560, -0.0907, -0.1526,  0.2028,\n",
       "            0.2537, -0.2767, -0.1782, -0.1604, -0.4009, -0.1286,  0.2928,\n",
       "           -0.2926,  0.1278,  0.2153,  0.0436, -0.1271,  0.0667],\n",
       "          [-0.0378, -0.1384, -0.0498,  0.0696, -0.1086, -0.1820,  0.2417,\n",
       "            0.3498, -0.2879, -0.2043, -0.1688, -0.3564, -0.1007,  0.1872,\n",
       "           -0.1550,  0.1140,  0.1996,  0.0561, -0.1113,  0.0832],\n",
       "          [-0.0278, -0.0248,  0.1232,  0.1575,  0.1664, -0.2251,  0.1326,\n",
       "            0.0955, -0.0706, -0.1592, -0.1755, -0.0476, -0.0557, -0.0165,\n",
       "            0.1652,  0.1688,  0.0959,  0.0956, -0.0610,  0.3632],\n",
       "          [-0.1378, -0.1285,  0.0155, -0.0526, -0.0218, -0.0912,  0.1423,\n",
       "            0.2052, -0.1434, -0.0656, -0.1386, -0.0523, -0.2035,  0.0392,\n",
       "           -0.1255,  0.0959,  0.0242,  0.0147, -0.0272,  0.0654]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " (tensor([[[-0.1661, -0.1376,  0.0195, -0.0819, -0.0258, -0.0465,  0.1400,\n",
       "             0.1331, -0.1401, -0.0166],\n",
       "           [-0.1378, -0.1285,  0.0155, -0.0526, -0.0218, -0.0912,  0.1423,\n",
       "             0.2052, -0.1434, -0.0656]],\n",
       "  \n",
       "          [[-0.1999, -0.1103, -0.3527,  0.1362, -0.2672,  0.1425,  0.0383,\n",
       "             0.0856, -0.0584,  0.1248],\n",
       "           [-0.1699, -0.4093, -0.1398,  0.3502, -0.3513,  0.1443,  0.2175,\n",
       "             0.0429, -0.1301,  0.0518]]], grad_fn=<StackBackward>),\n",
       "  tensor([[[-0.4799, -0.3292,  0.0436, -0.1681, -0.0584, -0.0895,  0.3573,\n",
       "             0.1977, -0.2924, -0.0227],\n",
       "           [-0.3961, -0.3123,  0.0349, -0.1060, -0.0492, -0.1794,  0.3690,\n",
       "             0.3051, -0.3038, -0.0906]],\n",
       "  \n",
       "          [[-0.6431, -0.2469, -0.5936,  0.2944, -0.5797,  0.5089,  0.0842,\n",
       "             0.1633, -0.1463,  0.3220],\n",
       "           [-0.3263, -0.7865, -0.4008,  0.7637, -1.0922,  0.3618,  0.3167,\n",
       "             0.0873, -0.2438,  0.1524]]], grad_fn=<StackBackward>)))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = model.embedding(torch.tensor([[1,1,2,3,1],[2,2,2,3,1]]))\n",
    "model.encoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0701,  0.4017, -1.2252,  1.1666,  0.8044, -0.6030, -1.6613,  0.0216,\n",
       "         -1.6670,  0.8180],\n",
       "        [-1.0701,  0.4017, -1.2252,  1.1666,  0.8044, -0.6030, -1.6613,  0.0216,\n",
       "         -1.6670,  0.8180],\n",
       "        [-0.7233, -1.8268, -1.2373,  0.8108,  1.9194, -1.7749, -0.1621, -0.1469,\n",
       "         -0.4632, -0.2646],\n",
       "        [ 0.6387,  0.4257, -0.7885,  0.2793, -0.9167,  1.5537,  0.8152, -1.3804,\n",
       "          0.6980, -0.1623],\n",
       "        [-1.0701,  0.4017, -1.2252,  1.1666,  0.8044, -0.6030, -1.6613,  0.0216,\n",
       "         -1.6670,  0.8180]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "for r in df['lower_line']:\n",
    "    vocab.update(r)\n",
    "for r in copas_t:\n",
    "    vocab.update(r)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ix = dict(zip(vocab, range(1, len(vocab)+1)))\n",
    "word2ix['<PAD>']= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['lower_line'].apply(lambda r: [word2ix[w] for w in r]).tolist()\n",
    "copas = copas_t.apply(lambda r: [word2ix[w] for w in r]).tolist()\n",
    "stances = (df['stance'] == 'Pro').astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertxu/.pyenv/versions/3.8.1/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = m.Model1(len(vocab) + 1, 100, 2)\n",
    "optimizer = optim.Adam(model.parameters(), weight_decay=.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 done.\n",
      "Test loss: 0.6637189388275146\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 1 done.\n",
      "Test loss: 0.6890787482261658\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 2 done.\n",
      "Test loss: 0.7924987077713013\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 3 done.\n",
      "Test loss: 0.9858576059341431\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 4 done.\n",
      "Test loss: 0.9146988987922668\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 5 done.\n",
      "Test loss: 0.7089576125144958\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 6 done.\n",
      "Test loss: 0.6765966415405273\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 7 done.\n",
      "Test loss: 0.6806909441947937\n",
      "Test acc: 0.367626404494382\n",
      "epoch 8 done.\n",
      "Test loss: 0.7165311574935913\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 9 done.\n",
      "Test loss: 0.7917943596839905\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 10 done.\n",
      "Test loss: 0.7002094388008118\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 11 done.\n",
      "Test loss: 0.8766077160835266\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 12 done.\n",
      "Test loss: 1.1894946098327637\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 13 done.\n",
      "Test loss: 2.525092363357544\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 14 done.\n",
      "Test loss: 3.401745080947876\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 15 done.\n",
      "Test loss: 1.8438029289245605\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 16 done.\n",
      "Test loss: 0.8329544067382812\n",
      "Test acc: 0.5158005617977528\n",
      "epoch 17 done.\n",
      "Test loss: 1.0734834671020508\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 18 done.\n",
      "Test loss: 0.7317956686019897\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 19 done.\n",
      "Test loss: 0.98270183801651\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 20 done.\n",
      "Test loss: 1.0072842836380005\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 21 done.\n",
      "Test loss: 1.4140686988830566\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 22 done.\n",
      "Test loss: 1.8499364852905273\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 23 done.\n",
      "Test loss: 1.0894314050674438\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 24 done.\n",
      "Test loss: 1.5511000156402588\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 25 done.\n",
      "Test loss: 1.0570080280303955\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 26 done.\n",
      "Test loss: 1.0894758701324463\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 27 done.\n",
      "Test loss: 1.3626292943954468\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 28 done.\n",
      "Test loss: 0.900672972202301\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 29 done.\n",
      "Test loss: 1.4332869052886963\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 30 done.\n",
      "Test loss: 2.417295217514038\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 31 done.\n",
      "Test loss: 1.0442302227020264\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 32 done.\n",
      "Test loss: 0.969385027885437\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 33 done.\n",
      "Test loss: 1.6355372667312622\n",
      "Test acc: 0.3648174157303371\n",
      "epoch 34 done.\n",
      "Test loss: 0.72564697265625\n",
      "Test acc: 0.6351825842696629\n",
      "epoch 35 done.\n",
      "Test loss: 1.3488346338272095\n",
      "Test acc: 0.6351825842696629\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c2ae7db9901a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_copas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstance_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.1/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tensor_copas = torch.tensor(pad_sents(copas))\n",
    "n_iter = 50\n",
    "for it in range(n_iter):\n",
    "    # for claim, stance in data_iterator(corrected, stances):\n",
    "    model.train()\n",
    "    for claim, stance in data_iterator(text, stances):\n",
    "        input_vec = torch.tensor(pad_sents(claim))\n",
    "        stance_vec = torch.tensor(stance)\n",
    "        probs = model(input_vec.to(device), tensor_copas.to(device))\n",
    "        loss = F.nll_loss(probs, stance_vec.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'epoch {it} done.')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_vec = torch.tensor(pad_sents(text))\n",
    "        stance_vec = torch.tensor(stances)\n",
    "        probs = model(input_vec.to(device), tensor_copas.to(device))\n",
    "        l = F.nll_loss(probs, stance_vec.to(device)).item()\n",
    "        print(f'Test loss: {l}')\n",
    "        \n",
    "        nwrong = np.count_nonzero(np.argmax(probs.cpu().numpy(), 1) - stances)\n",
    "        print(f'Test %wrong: {nwrong / len(stances)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
